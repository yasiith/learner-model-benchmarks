{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a6bd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#imports and load interaction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "df = pd.read_csv(\"../data/interactions.csv\")\n",
    "df = df.sort_values([\"student_id\", \"time_step\"]).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf14db7",
   "metadata": {},
   "source": [
    "DKT commonly uses \"item\" as the skill/question unit. Since the dataset already maps questions to concepts, we'll model questions as items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ad051",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Encode Items\n",
    "\n",
    "question_ids = sorted(df[\"question_id\"].unique())\n",
    "q2i = {q:i for i, q in enumerate(question_ids)}\n",
    "num_questions = len(question_ids)\n",
    "num_inputs = num_questions * 2  # (question, incorrect) or (question, correct)\n",
    "\n",
    "# Helper to build the DKT input index\n",
    "\n",
    "def encode_interaction(q_idx, correct):\n",
    "    return q_idx + (num_questions if correct else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42af1cb",
   "metadata": {},
   "source": [
    "We'll predict correctness at time t using history up to t-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab4376",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build sequences per student\n",
    "\n",
    "student_groups = []\n",
    "for sid, g in df.groupby(\"student_id\"):\n",
    "    g = g.sort_values(\"time_step\")\n",
    "    qs = [q2i[q] for q in g[\"question_id\"].tolist()]\n",
    "    cs = g[\"correct\"].astype(int).tolist()\n",
    "    student_groups.append((sid, qs, cs))\n",
    "\n",
    "len(student_groups), student_groups[0][0], len(student_groups[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df9073",
   "metadata": {},
   "source": [
    "Train/test split by student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7914133",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prevents leakage (a must-have for credibility)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "idx = np.arange(len(student_groups))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "split = int(0.8 * len(idx))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "train_students = [student_groups[i] for i in train_idx]\n",
    "test_students = [student_groups[i] for i in test_idx]\n",
    "\n",
    "len(train_students), len(test_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21696fcb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and Padding collate\n",
    "# sequences are padded in a batch and mask is used.\n",
    "\n",
    "class DKTDataset(Dataset):\n",
    "    def __init__(self, students):\n",
    "        self.students = students\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.students)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        _, qs, cs = self.students[i]\n",
    "        # X uses interactions up to t-1, y is correctness at t, target item is question at t\n",
    "        x = [encode_interaction(qs[t-1], cs[t-1]) for t in range(1, len(qs))]\n",
    "        target_q = [qs[t] for t in range(1, len(qs))]\n",
    "        y = [cs[t] for t in range(1, len(qs))]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(target_q, dtype=torch.long), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs, tq, ys = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    max_len = max(lengths).item()\n",
    "\n",
    "    x_pad = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    tq_pad = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    y_pad = torch.zeros(len(batch), max_len, dtype=torch.float32)\n",
    "    mask = torch.zeros(len(batch), max_len, dtype=torch.bool)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        L = lengths[i].item()\n",
    "        x_pad[i, :L] = xs[i]\n",
    "        tq_pad[i, :L] = tq[i]\n",
    "        y_pad[i, :L] = ys[i]\n",
    "        mask[i, :L] = True\n",
    "\n",
    "    return x_pad, tq_pad, y_pad, mask, lengths\n",
    "\n",
    "# DataLoaders:\n",
    "\n",
    "train_loader = DataLoader(DKTDataset(train_students), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(DKTDataset(test_students), batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2798d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DKT model\n",
    "\n",
    "class DKT(nn.Module):\n",
    "    def __init__(self, num_inputs, num_questions, emb_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_inputs, emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, num_questions)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        logits = self.out(out)  # (B, T, num_questions)\n",
    "        return logits\n",
    "\n",
    "# initialize:\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DKT(num_inputs=num_inputs, num_questions=num_questions).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19cc23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "# loss is computed only on valid (non-padded) timesteps.\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for x, tq, y, mask, lengths in loader:\n",
    "        x, tq, y, mask, lengths = x.to(device), tq.to(device), y.to(device), mask.to(device), lengths.to(device)\n",
    "\n",
    "        logits = model(x, lengths)  # (B,T,Q)\n",
    "        # pick logits of the target question at each timestep\n",
    "        target_logits = logits.gather(2, tq.unsqueeze(-1)).squeeze(-1)  # (B,T)\n",
    "\n",
    "        loss_mat = criterion(target_logits, y)  # (B,T)\n",
    "        loss = (loss_mat * mask.float()).sum() / mask.float().sum()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * mask.float().sum().item()\n",
    "        total_count += mask.float().sum().item()\n",
    "\n",
    "        probs = torch.sigmoid(target_logits)[mask].detach().cpu().numpy()\n",
    "        labels = y[mask].detach().cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    acc = accuracy_score(all_labels, [1 if p >= 0.5 else 0 for p in all_probs])\n",
    "    ll = log_loss(all_labels, all_probs)\n",
    "\n",
    "    return avg_loss, acc, ll\n",
    "\n",
    "    # train for a few epochs:\n",
    "\n",
    "    for epoch in range(1, 11):\n",
    "    tr_loss, tr_acc, tr_ll = run_epoch(train_loader, train=True)\n",
    "    te_loss, te_acc, te_ll = run_epoch(test_loader, train=False)\n",
    "    print(f\"Epoch {epoch:02d} | train acc={tr_acc:.3f} ll={tr_ll:.3f} | test acc={te_acc:.3f} ll={te_ll:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
